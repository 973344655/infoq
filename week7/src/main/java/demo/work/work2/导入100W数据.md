



## 0.说明

都是 mybatis + hikari连接池

CPU为i5-8265U 1.60GHz  4核8线程

## 1.单条插入

每次插入1条，插入100W次

```
for(int i=0; i<count; i++){
            OrderEntity entity = new OrderEntity(i);
            demoMapper.insertOne(entity);
        }
```

**耗时 4793S**

## 2.批量插入

利用mybatis foreach标签

### 2.1 一次1W,100次

```
List<OrderEntity> list = new ArrayList<>(10000);
        for(int i=0; i<count; i++){
            OrderEntity entity = new OrderEntity(i);
            list.add(entity);
            if(list.size() == 10000){
               demoMapper.insertBatch(list);
                list.clear();
            }
        }
```

**耗时 68S**

### 2.2 一次10W, 10次

将mysql最大传输数据大小设置为30M

```
set global max_allowed_packet =31457280;
```


```
 List<OrderEntity> list = new ArrayList<>(100000);
        for(int i=0; i<count; i++){
            OrderEntity entity = new OrderEntity(i);
            list.add(entity);
            if(list.size() == 100000){
                demoMapper.insertBatch(list);
                list.clear();
            }
        }
```

**耗时 85S**

虽然传输次数少了，但是因为数据包太大，传输时间及执行时间，反而比一次1W更长，说明并不是一次传输数据越多越好


### 2.3 一次1W,使用线程池

基于上面，所以选择每次1W条

```
  /**
     * 使用线程池，每次插入1W条
     */
    public void insertBatch2(){
//        ExecutorService executor = Executors.newFixedThreadPool(10);
        ThreadPoolExecutor executor = new ThreadPoolExecutor(
                Runtime.getRuntime().availableProcessors(),
                10,
                1L,
                TimeUnit.MINUTES,
                new LinkedBlockingDeque<>(100),
                new ThreadPoolExecutor.CallerRunsPolicy());

        long start = System.currentTimeMillis();
        System.out.println(start);

        for(int i=0; i<100; i++){
            List<OrderEntity> list = new ArrayList<>(10000);
            for(int j=0; j<10000; j++){
                OrderEntity entity = new OrderEntity(i*10000 + j);
                list.add(entity);

                }
            executor.submit(() -> {
                System.out.println(Thread.currentThread().getName());
                long t1 = System.currentTimeMillis();

                demoMapper.insertBatch(list);
                long t2 = System.currentTimeMillis();

                System.out.println("used: " + (t2-t1));
            });

        }

        executor.shutdown();
        try {
            executor.awaitTermination(3L, TimeUnit.MINUTES);
        }catch (Exception e){
            e.printStackTrace();
        }

        long end = System.currentTimeMillis();
        System.out.println(end);
        System.out.println("耗时: " + (end - start));
    }

```

**耗时:  42s**

## 3.直接导入csv文件

生成100W条数据的csv文件，文件大小147M

**耗时: 71秒**

比批量导入还慢？？


## 4.总结

按照视频里面讲的几种方法，测试了一下，在我这环境下最快42S







